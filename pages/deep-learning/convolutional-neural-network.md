# 卷积神经网络

> **卷积神经网络**（Convolutional Neural Network，CNN）被用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。

::: danger 警告

该页面尚未完工!

:::

::: details 目录

[[toc]]

:::

## 整体结构

CNN 和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，CNN 中新出现了**卷积层**（Convolution 层）和**池化层**（Pooling 层）。

之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为**全连接**（fully-connected），并用 Affine 层实现了全连接层。

![基于全连接层（Affine 层）的网络的例子](/images/deep-learning/convolutional-neural-network/fully-connected.png)

> 全连接的神经网络中，Affine 层后面跟着激活函数 ReLU 层（或者 Sigmoid 层）。
>
> 这里堆叠了 4 层 “Affine-ReLU” 组合，然后第 5 层是 Affine 层，最后由 Softmax 层输出最终结果（概率）。

那么，CNN 会是什么样的结构呢：

![基于 CNN 的网络的例子](/images/deep-learning/convolutional-neural-network/cnn.png)

> 新增了 Convolution 层和 Pooling 层（用灰色的方块表示）

CNN 中新增了 Convolution 层和 Pooling 层。CNN 的层的连接顺序是 “Convolution-ReLU-(Pooling)”（Pooling 层有时会被省略）。这可以理解为之前的 “Affine - ReLU” 连接被替换成了 “Convolution-ReLU-(Pooling)” 连接。

还需要注意的是，靠近输出的层中使用了之前的 “Affine-ReLU” 组合。此外，最后的输出层中使用了之前的 “Affine-Softmax” 组合。这些都是一般的 CNN 中比较常见的结构。

## 卷积层

### 全连接层存在的问题

之前介绍的全连接的神经网络中使用了全连接层（Affine 层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。

但这样的话数据的形状被 “忽视” 了。比如，输入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。

使用了 MNIST 数据集的例子中，输入图像就是 1 通道、高 28 像素、长 28 像素的（1,28,28）形状，但却被排成 1 列，以 784 个数据的形式输入到最开始的 Affine 层。

图像是 3 维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3 维形状中可能隐藏有值得提取的本质模式。

但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。

而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。因此，在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。

另外，CNN 中，有时将卷积层的输入输出数据称为**特征图**（featuremap）。其中，卷积层的输入数据称为**输入特征图**（input feature map），输出数据称为**输出特征图**（output feature map）。

### 卷积运算

卷积层进行的处理就是**卷积运算**。卷积运算相当于图像处理中的 “滤波器运算”。

我们来看一个具体的例子：

卷积运算对输入数据应用滤波器。在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是 (4, 4)，滤波器大小是 (3, 3)，输出大小是 (2, 2)。

![卷积运算的例子](/images/deep-learning/convolutional-neural-network/convolution-operation.png)

> 仅应用滤波器

现在来解释一下图中的卷积运算例子都进行了什么样的计算：

![卷积运算的计算顺序](/images/deep-learning/convolutional-neural-network/convolution-operation-calculation.png)

对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。这里所说的窗口是指图中灰色的 3×3 的部分，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为**乘积累加运算**）。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。

在全连接的神经网络中，除了权重参数，还存在偏置。CNN 中，滤波器的参数就对应之前的权重。并且，CNN 中也存在偏置：

![卷积运算的偏置：向应用了滤波器的元素加上某个固定值（偏置）](/images/deep-learning/convolutional-neural-network/convolution-operation-bias.png)

上图在应用了滤波器的基础上加上了偏置。偏置通常只有 1 个（1×1），这个值会被加到应用了滤波器的所有元素上。

### 填充

在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如 0 等），这称为**填充**（padding），是卷积运算中经常会用到的处理。

比如，对大小为 (4, 4) 的输入数据应用了幅度为 1 的填充。“幅度为 1 的填充” 是指用幅度为 1 像素的 0 填充周围：

![卷积运算的填充处理：向输入数据的周围填入 0](/images/deep-learning/convolutional-neural-network/convolution-operation-padding.png)

> 图中用虚线表示填充，并省略了填充的内容 “0”

通过填充，大小为 (4, 4) 的输入数据变成了 (6, 6) 的形状。然后，应用大小为 (3, 3) 的滤波器，生成了大小为 (4, 4) 的输出数据。

如果将填充设为 2，则输入数据的大小变为 (8, 8)；如果将填充设为 3，则大小变为 (10, 10)。

::: tip 为什么要使用填充

使用填充主要是为了调整输出的大小。

比如，对大小为 (4, 4) 的输入数据应用 (3, 3) 的滤波器时，输出大小变为 (2, 2)，相当于输出大小比输入大小缩小了 2 个元素。

这在反复进行多次卷积运算的深度网络中会成为问题。因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。

为了避免出现这样的情况，就要使用填充。

在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4,4)，输出大小也保持为原来的(4,4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。

:::

### 步幅

应用滤波器的位置间隔称为**步幅**（stride）。

之前的例子中步幅都是 1，如果将步幅设为 2，则应用滤波器的窗口的间隔变为 2 个元素：

![步幅为2的卷积运算的例子](/images/deep-learning/convolutional-neural-network/convolution-operation-stride.png)

> 对输入大小为 (7, 7) 的数据，以步幅 2 应用了滤波器。通过将步幅设为 2，输出大小变为 (3, 3)。

像这样，步幅可以指定应用滤波器的间隔。

综上，**增大步幅后，输出大小会变小**。而**增大填充后，输出大小会变大**。

假设输入大小为 ($H$,$W$)，滤波器大小为 ($FH$,$FW$)，输出大小为 ($OH$,$OW$)，填充为 $P$，步幅为 $S$，则输出大小可通过下式进行计算：

$$OH = \frac{H + 2P - FH}{S} + 1$$

$$OW = \frac{W + 2P - FW}{S} + 1$$

::: details 具体示例

输入大小：(4, 4)；填充：1；步幅：1；滤波器大小：(3, 3)

$$OH = \frac{4 + 2 \times 1 - 3}{1} + 1 = 4$$

$$OW = \frac{4 + 2 \times 1 - 3}{1} + 1 = 4$$

输入大小：(7, 7)；填充：0；步幅：2；滤波器大小：(3, 3)

$$OH = \frac{7 + 2 \times 0 - 3}{2} + 1 = 3$$

$$OW = \frac{7 + 2 \times 0 - 3}{2} + 1 = 3$$

输入大小：(28, 31)；填充：2；步幅：3；滤波器大小：(5, 5)

$$OH = \frac{28 + 2 \times 2 - 5}{3} + 1 = 10$$

$$OW = \frac{31 + 2 \times 2 - 5}{3} + 1 = 11$$

:::

::: warning 注意

虽然只要代入值就可以计算输出大小，但是所设定的值必须使 $\frac{W + 2P - FW}{S}$ 和 $\frac{H + 2P - FH}{S}$ 分别可以除尽。

当输出大小无法除尽时（结果是小数时），需要采取报错等对策。

根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。

:::

### 3 维数据的卷积运算

之前的卷积运算的例子都是以有高、长方向的 2 维形状为对象的。

但是，图像是 3 维数据，除了高、长方向之外，还需要处理通道方向。

这里，我们按照与之前相同的顺序，看一下对加上了通道方向的 3 维数据进行卷积运算的例子：

![对 3 维数据进行卷积运算的例子](/images/deep-learning/convolutional-neural-network/convolution-operation-3d.png)

> 对 3 维数据进行卷积运算的例子

![对 3 维数据进行卷积运算的计算顺序](/images/deep-learning/convolutional-neural-network/convolution-operation-3d-calculation.png)

> 对 3 维数据进行卷积运算的计算顺序

和 2 维数据时相比，可以发现纵深方向（通道方向）上特征图增加了。

通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。

::: warning 注意

在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。

:::

### 结合方块思考

将数据和滤波器结合长方体的方块来考虑，3 维数据的卷积运算会很容易理解。

![结合方块思考卷积运算](/images/deep-learning/convolutional-neural-network/convolution-operation-3d-block.png)

> 方块是如图所示的 3 维长方体。
>
> 请注意方块的形状

把 3 维数据表示为多维数组时，书写顺序为（channel, height, width）。比如，通道数为 $C$、高度为 $H$、长度为 $W$ 的数据的形状可以写成（$C$, $H$, $W$）。

滤波器也一样，要按（channel, height, width）的顺序书写。比如，通道数为 $C$、滤波器高度为 $FH$（FilterHeight）、长度为 $FW$（Filter Width）时，可以写成（$C$, $FH$, $FW$）。

在这个例子中，数据输出是 1 张特征图。换句话说，就是通道数为 1 的特征图。

如果要在通道方向上也拥有多个卷积运算的输出，就需要用到多个滤波器（权重）：

![基于多个滤波器的卷积运算的例子](/images/deep-learning/convolutional-neural-network/convolution-operation-3d-multiple-filters.png)

通过应用 $FN$ 个滤波器，输出特征图也生成了 $FN$ 个。如果将这 $FN$ 个特征图汇集在一起，就得到了形状为 ($FN$, $OH$, $OW$)的方块。将这个方块传给下一层，就是 CNN 的处理流。

因此，关于卷积运算的滤波器，也必须考虑滤波器的数量。作为 4 维数据，滤波器的权重数据要按 (output_channel, input_channel, height, width) 的顺序书写。比如，通道数为 3、大小为 5×5 的滤波器有 20 个时，可以写成 (20, 3, 5, 5)。

如果进一步追加偏置的加法运算处理：

![卷积运算的处理流（追加了偏置项）](/images/deep-learning/convolutional-neural-network/convolution-operation-3d-block-with-bias.png)

> 每个通道只有一个偏置。这里，偏置的形状是 ($FN$, 1, 1)，滤波器的输出结果的形状是 ($FN$, $OH$, $OW$)。

这两个方块相加时，要对滤波器的输出结果 ($FN$, $OH$, $OW$) 按通道加上相同的偏置值。

### 批处理

神经网络的处理中进行了将输入数据打包的批处理。之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对 mini-batch 的对应。

我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为 4 维数据。具体地讲，就是按 (batch_num, channel, height, width) 的顺序保存数据。

比如，将之前的处理改成对 N 个数据进行批处理：

![卷积运算的处理流（批处理）](/images/deep-learning/convolutional-neural-network/convolution-operation-3d-batch.png)

图中的批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为 4 维的形状在各层间传递。

::: warning 注意

网络间传递的是 4 维数据，对这 N 个数据进行了卷积运算。

也就是说，批处理将 N 次的处理汇总成了 1 次进行。

:::

## 池化层

池化是缩小高、长方向上的空间的运算。

比如，进行将 2×2 的区域集约成 1 个元素的处理，缩小空间大小。

![池化的处理顺序](/images/deep-learning/convolutional-neural-network/pooling-calculation.png)

> 按步幅 2 进行 2×2 的 $Max$ 池化的处理顺序
>
> “$Max$ 池化” 是获取最大值的运算
>
> “2×2” 表示目标区域的大小

如图所示，从 2×2 的区域中取出最大的元素。此外，这个例子中将步幅设为了 2，所以 2×2 的窗口的移动间隔为 2 个元素。一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3×3 的窗口的步幅会设为 3，4×4 的窗口的步幅会设为 4 等。

::: details 关于池化

除了 Max 池化之外，还有 Average 池化等。相对于 Max 池化是从目标区域中取出最大值，Average 池化则是计算目标区域的平均值。在图像识别领域，主要使用 Max 池化。

:::

### 池化层的特征

池化层有以下特征：

- **没有要学习的参数**：池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。

- **通道数不发生变化**：经过池化运算，输入数据和输出数据的通道数不会发生变化。如下图所示，计算是按通道独立进行的。

![池化中通道数不变](/images/deep-learning/convolutional-neural-network/pooling-channel-unchanged.png)

- **对微小的位置变化具有鲁棒性（健壮）**：输入数据发生微小偏差时，池化仍会返回相同的结果。比如，3×3 的池化的情况下，池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）

![输入数据在宽度方向上只偏离 1 个元素时，输出仍为相同的结果](/images/deep-learning/convolutional-neural-network/pooling-robust.png)

## 卷积层和池化层的实现

::: danger 警告

该部分尚未完工!

:::

## 小结

::: danger 警告

该部分尚未完工!

:::

::: details 专有名词

- **卷积神经网络**：

- **卷积层（Convolution 层）**：

- **池化层（Pooling 层）**：

- **全链接**：神经网络中，相邻层的所有神经元之间都有连接

- **特征图**：卷积层中输入输出的数据

- **卷积运算**：卷积层进行的处理

- **滤波器**：卷积运算中用于初步处理数据的部分

- **填充**：卷积运算中，向输入数据的周围填入固定的数据（比如 0 等），以调整输出的大小，增大填充后输出大小会变大

- **步幅**：卷积运算中，应用滤波器的位置间隔，增大步幅后输出大小会变小

:::
