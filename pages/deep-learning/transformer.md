# Transformer

> Transformer 是一种专门设计用来处理序列数据（例如句子、语音、时间序列）的模型

::: danger 警告

该页面尚未完工!

:::

::: details 目录

[[toc]]

:::

## 什么是 Transformer

::: danger 警告

该部分尚未完工!

:::

## 词嵌入

### 处理文字

文字是相当好的，我们可以用它们来传达各种酷炫的想法。

不幸的是，许多机器学习算法，包括神经网络，并不擅长处理文字。

所以，如果我们想把文字输入神经网络或其他一些机器学习算法，我们需要一种方法将文字转换成数字。

一种非常简单的将文字转换成数字的方法就是为每个词分配一个随机数字。

::: details 具体示例

如果有人刚看完热门电影《Troll 2》并说："Troll 2 is great"，我们可以为每每个词分配一个随机数字：

- Troll 2 --> 12

- is --> -3.05

- great --> 4.2

如果下一个人说："Trolls 2 is awesome"，那么我们可以重新使用我们已经为 "Trolls 2" 和 "is" 选择的随机数字，并为新词 "awesome" 分配一个新的随机数字：

- Troll 2 --> 12 <-- Troll 2

- is --> -3.05 <-- is

- great --> 4.2

- awesome --> -32.1

:::

理论上这是可行的。

但这意味着尽管 "great" 和 "awesome" 意思相似并且用法相似，它们关联的数字却大不相同。

这意味着神经网络在训练中可能需要更多的复杂性。（因为学会正确处理词 "great" 不会帮助神经网络正确使用词 "awesome"）

所以如果用法相似的相似词能够被赋予相似的数字就好了，这样学会使用一个词就能同时帮助学会使用另一个词。而且因为同一个词可以在不同的语境中使用，或变成复数、或以其他方式使用，为每个词分配多个数字可能会很好。这样神经网络可以更容易地适应不同的语境。

::: details 具体示例

例如，词 "great" 可以用在积极的方式："StatQuest is great!"

它也可以以讽刺负面的方式使用："My cellphone's broken, great."

所以，如果我们有一个数字，能够记录 "great" 使用的积极方式和另一个数字来记录负面方式，那将很棒。

:::

好消息是，我们可以让一个超级简单的神经网络为我们完成所有工作。使用神经网络的优势是它可以使用训练数据集中词汇的上下文，来优化可用于嵌入的权重。

**词嵌入**（Word Embedding）是自然语言处理（NLP）中的一项核心技术。它是一种将文字符号（单词、词组、字符）转换为计算机能够处理的数值形式（即向量） 的方法。更重要的是，这种转换不是随机的，而是让转换后的数值向量能够捕捉到词语的语义和上下文信息。

### 数字与词的关联

假设我们有两个短语："Trolls 2 is great" 和 "Gymkata 2 is great"。

为了创建一个神经网络来弄清楚我们应该将哪些数字与每个词关联，我们首先为每个独特的词创建输入：

<img src="/images/deep-learning/transformer/words-input.png" alt="词的输入" width="60" />

> 在这个例子中我们的训练数据中有四个独特的词，因此我们有四个输入

现在我们将每个输入链接到至少一个激活函数：

<img src="/images/deep-learning/transformer/words-activation.png" alt="词的激活" width="160" />

> 这个激活函数使用恒等函数

激活函数的数量对应我们想要与每个词关联的数字数量，并且这些连接上的权重最终会是我们与每个词关联的数字。

如果我们想要与每个词关联两个数字，这意味着我们将使用两个激活函数，并且连接到第二个激活函数的权重将是与每个词关联的另一个数字：

<img src="/images/deep-learning/transformer/words-activation-two.png" alt="两个激活函数" width="170" />

像往常一样，这些权重一开始都是随机值，这些权重将通过反向传播进行优化。

现在，为了进行反向传播，我们必须做出预测。所以我们将使用输入词来预测短语中的下一个词。

如果短语是 "Troll 2 is great"，那么我们可以使用词 "Troll 2" 来预测词 "is"。

换句话说，如果输入词是 "Troll 2"，我们通过在 "Troll 2" 的输入中放置 1 来指示这一点，并在所有其他输入中放置 0。那么我们希望下一个词 "is" 的输出具有最大值：

<div style="display: flex;align-items: center;">

<img src="/images/deep-learning/transformer/words-prediction-1000.png" alt="输入 Troll 2" width="60" />

——> 经过计算 ——>

<img src="/images/deep-learning/transformer/words-result-0100.png" alt="输出 is" width="60" />

</div>

同理，如果输入词是 "is"，这意味着 "is" 的输入是 1，所有其他输入都是 0，那么我们希望下一个词 "great" 的输出具有最大值。

<div style="display: flex;align-items: center;">

<img src="/images/deep-learning/transformer/words-prediction-0100.png" alt="输入 is" width="60" />

——> 经过计算 ——>

<img src="/images/deep-learning/transformer/words-result-0010.png" alt="输出 great" width="60" />

</div>

为了做出这些预测，我们将激活函数连接到输出，并在这些连接上添加随机初始化值的权重。

然后我们通过 Softmax 函数运行输出，因为我们有多个分类输出。

![预测结果](/images/deep-learning/transformer/words-softmax.png)

这意味着我们可以使用交叉熵损失函数进行反向传播。

再次强调，目标是训练这个神经网络，以便它能正确预测短语中的下一个词。

### 可视化

假设在训练之前，该神经网络能正确处理 "Troll 2" 的输入计算并正确预测下一个词 "is"：

![正确预测](/images/deep-learning/transformer/words-prediction-true.png)

但还无法正确处理 "is" 的输入计算并预测：

![错误预测](/images/deep-learning/transformer/words-prediction-false.png)

所以我们需要训练这个神经网络。

在我们优化所有权重之前，我们可以在图表上绘制每个词

![词的图表](/images/deep-learning/transformer/words-graph.png)

> 图表的 x 轴是连接到顶部激活函数的权重值，y 轴是连接到底部激活函数的权重值

我们现在看到，词 "Troll 2" 和 "Gymkata" 现在并不接近相比训练数据中的其他词汇。

然而，由于这两个词在训练数据中出现在相同的上下文中，我们希望反向传播会使它们的权重变得更加相似。

![词的图表 2](/images/deep-learning/transformer/words-graph-2.png)

当我们使用新的权重绘制词汇时，我们看到 "Troll 2" 和 "Gymkata" 现在相对于其他词汇更接近。

此时再进行 "Troll 2" 和 "Gymkata" 的预测，我们会会得到我们想要的结果。

总结一下：

- 首先，我们不是随机分配数字给词汇，而是训练一个相对简单的神经网络来为我们分配数字。这可以使相似的词汇最终具有相似的嵌入。

- 最后，具有相似嵌入的相似词汇意味着训练一个处理语言的神经网络更容易，因为学习一个词的使用有助于学习如何使用相似的词。

截至目前，我们展示了我们可以训练一个神经网络来预测每个短语中的下一个词。但是仅仅预测下一个词并没有给我们提供足够的上下文来理解每个词。

所以现在让我们学习新的策略使得用于包含更多的上下文。

### Word2Vec

Word2Vec 是一种流行的创建词嵌入的方法，可以用于包含更多的上下文。

Word2Vec 包含两种方法： **连续词袋**（Context Bag of Words）和 **跳跃模型**（SkipGram）。

连续词袋方法通过使用周围的词来预测中间发生的词来增加语境。

::: details 具体示例

例如，连续词袋方法可能使用词 "Troll 2" 和 "great" 来预测它们之间的词，即 "is"。

:::

跳跃模型方法通过使用中间的词来预测周围的词来增加语境。

::: details 具体示例

例如，跳跃模型方法可以使用词 "is" 来预测周围的词 "Troll 2"、"great" 和 "Gymkata"。

:::

最后，在结束之前，请知道在实践中，人们通常不是仅使用两个激活函数来为每个词语创建两个词嵌入。人们通常使用 100 个或更多的激活函数来为每个词创建大量的嵌入。而且不是使用两个句子进行训练，它们使用整个维基百科。

因此，Word2Vec 不是只有四个单词和短语的词汇量，而是可能拥有大约 300 万词的词汇表。

![Word2Vec](/images/deep-learning/transformer/word2vec.png)

因此，我们需要优化的这个神经网络中的权重总数是 300 万词汇，至少乘以 100（每个词到激活函数的权重的数量），再乘以 2（从激活函数到输出的权重也是 300 万乘以 100），总共 6 亿个权重。因此训练可能会很慢。

然而，Word2Vec 加速的一种方式是采用**负采样**，随机选择一部分我们不想预测哪些用于优化的单词。

::: details 具体示例

例如，假设我们想要预测单词 "aardvark" 来预测单词 "A"。

这意味着只有单词 "aardvark" 中有一个 1，而所有其他单词都是 0。

<img src="/images/deep-learning/transformer/words-prediction-aardvark.png" alt="预测单词 aardvark" width="60" />

这意味着我们可以忽略来自除了 "aardvark" 之外所有其他单词的权重，因为其他单词将他们的权重乘以 0。

![忽略权重](/images/deep-learning/transformer/words-ignore-weights.png)

这单独就从这个优化步骤中移除了接近 3 亿个权重。

然而，激活函数之后我们仍然有 3 亿个权重。

因为我们想预测单词 "A"，不想预测 "aardvark"、"abandon" 和所有其他单词。所以在这个例子中，让我们想象 Word2Vec 随机选择 "abandon" 作为我们不想预测的单词。

> 实际上，Word2Vec 会选择我们不想预测的 2 至 20 个单词，这个例子中我们只选择了 1 个 "abandon"

所以现在，Word2Vec 只使用 "A" 和 "abandon" 的输出值。这意味着在这轮反向传播中，我们可以忽略导致所有其他可能输出的权重：

![忽略权重 2](/images/deep-learning/transformer/words-ignore-weights-2.png)

所以最终，在这个神经网络中总共有 6 亿个权重，我们每步只优化 300 个。这是 Word2Vec 有效创建大词汇量中每个单词的大量词嵌入的一种方式。

:::

## 编码&解码神经网络

现在你面前有一种序列（语言句子，DNA 序列等），需要翻译成另一种的序列。

这两个问题和许多其他类似问题称为**序列到序列问题**（sequence-to-sequence, seq2seq）。

解决 seq2seq 的一种方法是**编码器-解码器模型**。

### 编码器

我们的目标是创造一个可以将英语句子翻译成西班牙语的编码器-解码器模型。

但是首先，这很明显，在英语中不是所有的句子长度都相同（"Let's go" 与 "My name is StatSquatch"），所以我们需要可以做到让不同长度的句子作为输入。

同样，并非所有西班牙语句子的长度都相同，所以我们需要可以做到生成不同长度的句子作为输出。

最后，英语句子的西班牙语翻译比原先句子可以有不同的长度。

::: details 具体示例

比如两个单词的英语句子 "Let's go!" 翻译成一个西班牙语单词的句子 "Vamos."

:::

所以我们需要 seq2seq 编码器-解码器模型能够处理可变的输入和可变输出长度。

好消息是我们已经知道如何使用 LSTM （长短期记忆单元）来处理具有可变长度的输入和输出。

::: details 具体示例

例如，如果输入的句子是 "Let's go!"，然后我们把 "Let's" 放入 LSTM 的输入中，然后展开 LSTM，然后将 "go" 插入第二个输入

![处理 "Let's go!"](/images/deep-learning/transformer/lstm-letsgo.png)

:::

但我们不能直接把单词塞进神经网络，反而，我们使用词嵌入将单词转换为数字。因为词汇中包含了单词和符号，我们将词汇表中的各个元素作为 **tokens**。

::: details 具体示例

> 为了使示例相对简单，我们的编码器解码器的英语词汇只有三个字 "Let's"、"to"、"go"，实际应有大量的词汇
>
> < EOS > 代表句子结束
>
> 在此示例中，我们只是在创造每个 token 两个嵌入值，而不是数百或数千

![微型词嵌入](/images/deep-learning/transformer/word-embedding-small.png)

:::

现在我们有了输入的嵌入层词汇表，我们可以把它放在 LSTM 的输入前面

![连接词嵌入与 LSTM](/images/deep-learning/transformer/word-embedding-to%20lstm.png)

现在当我们输入句子 "Let's go!"，我们在 "Let's" 输入中输入 1 且其他都为 0；然后我们展开 LSTM 和嵌入层，并在 "go" 的输入中输入 1 且其他都为 0

::: details 注意

当我们展开 LSTM 和嵌入层时，我们重复使用完全相同的权重的偏置，无论我们展开多少次。

换句话说，LSTM 单元和嵌入层中用于表示 "Let's" 这个词的权重和偏置和我们对 "go" 这个词使用的权重和偏置完全一样。

:::

现在，理论上，这就是我们对输入句子进行编码所需要做的全部工作。

然而，在实践中，为了有更多的权重的偏置使模型适合我们的数据，人们经常在输入中添加额外的 LSTM 单元。

![两层 LSTM](/images/deep-learning/transformer/two-lstm.png)

> 为了保持简单，我们只需在此阶段添加一个额外的 LSTM 单元
>
> 这意味着两个单词的嵌入值用作两个不同 LSTM 单元的输入值，这两个 LSTM 单元有自己单独的权重和偏置集

现在，添加更多的权重和偏置使模型适合我们的数据，人们经常添加额外的 LSTM 层。

为了说明这是如何工作的，我们将在编码其中再添加一层 LSTM 层，这意味着第二层中展开的 LSTM 单元以第一层中展开的 LSTM 单元的输出值、短期记忆或隐藏层作为输入。

![更多 LSTM](/images/deep-learning/transformer/more-lstm.png)

::: details 注意

就像两个嵌入值如何被用作第一层中两个 LSTM 单元的输入一样，来自第一层的每个单元格的两个输出（短期记忆或隐藏状态）都作为第二层的 LSTM 单元的输入

实际应用中不止两层 LSTM，每层也不止 2 个 LSTM 单元

:::

最后，唯一要做的就是初始化长期和短期记忆，cell 和隐藏状态。

现在我们完成了创建编码器-解码器模型的**编码器**部分。

![编码器](/images/deep-learning/transformer/encoder.png)

总结一下，在此示例中，我们有两层 LSTM，每层分别有两个 LSTM 单元。

本质上，编码器对输入的句子进行编码，变成长期记忆和短期记忆的集合，也称为细胞状态和隐藏状态。

最后的长期和短期记忆（细胞状态和隐藏状态）来自编码器中 LSTM 单元的两层，被称为**上下文向量**。因此，编码器对输入句子进行编码，"Let's go" 变成上下文向量。

### 解码器

现在我们需要解码上下文向量。

所以我们要做的第一件事是从上下文向量中连接出长期记忆和短期记忆（隐藏状态的细胞），使其传输到一组新的 LSTM。

![新的 LSTM](/images/deep-learning/transformer/new-lstm-layer.png)

就像编码器一样，有两层，每层有两个单元。

::: warning 注意

要明确解码器中的 LSTM 是与编码器中的不同，并有自己独立的权重和偏置

:::

无论如何，上下文向量用于初始化解码器 LSTM 中的长期和短期记忆（隐藏状态单元）。解码器的最终目标是解码上下文向量作为输出句子。

![解码器第 1 层的输入](/images/deep-learning/transformer/decoder-input-1.png)

就像在编码器中一样，第一层 LSTM 单元的输入来自嵌入层，但现在的嵌入层创建了西班牙语单词的嵌入值。

::: details 编码器与解码器的嵌入层

![两个嵌入层](/images/deep-learning/transformer/two-embedding-layer.png)

> 左边是我们在编码器中使用的嵌入层
>
> 右边是我们在解码器中使用的嵌入层

它们有不同的输入单词和样本或标记，以及不同的权重，从而导致每个标记不同的嵌入值。

:::

因为我们刚刚完成对英语句子 "Let's go" 的编码，解码器从 EOS（句末）的嵌入值开始，在这种情况下，我们使用 EOS token 开始解码（有的会使用 SOS 作为句子开头）。

反正，解码器使用两层 LSTM 进行数学计算。

### 全连接

解码器 LSTM 单元顶层的输出值通过格外的权重和偏置 transform 到全连接层中。

![解码器到全连接层](/images/deep-learning/transformer/decoder-to-fullconnected.png)

这个全连接层具有来自顶层 LSTM 单元的两个值的两个输入，且输出对应西班牙词汇表中的每个词汇。

在这两者之间，我们在每个输入和输出之间都有权重的偏置的联系，然后我们通过 softmax 函数运行全连接层的输出来提取输出词：

![全连接层](/images/deep-learning/transformer/fullconnected.png)

现在回到完整的编码器-解码器模型，我们就可以看到 softmax 函数的输出是 "vamos"，西班牙语 "Let's go" 的翻译。

### 继续解码

目前为止，翻译是正确的。但直到输出 EOS token 解码器才会停止。

所以我们把 "vamos" 插入解码器展开的嵌入层，并扩展 LSTM 单元，然后将输出值运行到相同的全连接层：

![解码器最终输出](/images/deep-learning/transformer/decoder-output.png)

下一个预测 token 是 EOS，这意味着我们翻译了英文句子 "Let's go" 到正确的西班牙语句子 "Vamos"。

总结一下解码器阶段，由编码器展开的两层 LSTM 单元创建的上下文向量用于初始化解码器中的 LSTM，LSTM 的输入来自以 EOS 开始的输出词嵌入层，EOS 的输出又决定下一层的输入。解码器将继续预测单词直到它预测出 EOS token 或者达到某个最大输出长度。

::: warning 注意

通过将编码器与解码器解耦输入文本和翻译后的输出文本可以有不同的长度

且实际应用中的全连接层的输入输出都要更多

:::

### 训练

就像所有神经网络一样，所有这些权重和偏置都是使用反向传播进行训练的。

在预测时，解码器每一层的输入来源于上一层的输出。

但在训练编码器-解码器时，我们不使用预测的 token 作为解码器 LSTM 的输入，而是使用已知的、正确的标记。

换句话说，如果第一个预测的标记时西班牙语单词 "y"，在英语中翻译为 "and"，是错误的词，那么在训练期间我们仍然会使用 "vamos" 将正确的西班牙语单词作为无规则 LSTM 的输入。

![解码器的训练](/images/deep-learning/transformer/decoder-trainning-example.png)

另外，在训练过程中，不仅仅是预测标记直到解码器预测出 EOS token，每个输出短语在已知短语结束的地方结束。

换句话说，即使第 2 个预测标记是西班牙语单词 "ir" 而不是正确的标记 EOS，在训练期间，我们仍然会停止预测额外的 token。停在已知的短语长度，而不是将预测的 token 用于所有事情。

## 位置编码

::: danger 警告

该部分尚未完工!

:::

位置编码是一种为 Transformer 模型提供词语在序列中 "顺序" 和 "位置" 信息的技术。 它是解决 Transformer 核心缺陷——"自身不具备理解顺序能力"——的关键。

::: details Transformer 的 "先天缺陷"

Transformer 的核心是自注意力机制。它的工作方式是：当处理一个句子时，它会同时关注句子中的所有词，然后根据重要性加权求和。

但这种 "同时处理" 的方式，意味着模型天然地丢失了词的顺序信息。

例如，对于模型来说，句子 "我爱中国" 和 "中国爱我" 的输入（在没有位置信息时）是一模一样的，因为都是由 "我"、"爱"、"中国" 这三个词组成的集合。

但显然，这两个句子的含义天差地别。顺序是语言理解的基础。

:::

我们必须显式地告诉模型每个词在序列中的位置，否则它就无法理解语言的逻辑结构。位置编码就是完成这个任务的 "位置说明书"。

## 自注意力

::: danger 警告

该部分尚未完工!

:::

**自注意力机制**是一种让序列中的每一个元素（例如句子中的每一个词）都能与序列中所有其他元素（包括它自己）进行直接交互，并根据相关性动态分配权重，从而生成一个新的、融合了全局上下文信息的表示向量的方法。

一般来说，自注意力机制的原理是通过检查每个词与句子中所有词（包括它自己）的相似度。

### 动态加权聚合

想象你在阅读这句话：

> "The animal didn't cross the street because it was too tired."
>
> （那只动物没有过马路，因为它太累了。）

作为人类，当你看到 "it" 时，你会立刻将注意力聚焦到 "animal" 上，而不是 "street"。自注意力机制所做的就是自动化这个过程。

所以，我们的目标就是为句子中的每个词（如 "it"）计算一个新的表示向量，通过考察句中所有词与 "it" 的相关性，对它们的值进行加权求和，得到新生成的 "it" 的向量会包含关于"animal" 的强烈信息，从而帮助模型理解 "it" 指代的是什么。

## 残差链接

::: danger 警告

该部分尚未完工!

:::

## 解码器

::: danger 警告

该部分尚未完工!

:::

## 小结

::: danger 警告

该部分尚未完工!

:::

::: details 专有名词

- **词嵌入**：使得 transformer 能够将单词编码成数字

- **位置编码**：使得 transformer 能够编码单词的位置

- **自注意力**：使得 transformer 能够编码单词之间的关系

- **残差链接**：使得 transformer 可以相对容易和快速地并行训练

:::
