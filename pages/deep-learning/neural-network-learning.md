# 神经网络的学习

::: danger 警告
该页面尚未完工!
:::

## 目录

[[toc]]

## 从数据中学习

神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。

### 数据驱动

现在我们来思考一个具体的问题，比如如何实现数字“5”的识别，我们的目标是实现能区别是否是 5 的程序。

![手写数字5的例子](/images/deep-learning/neural-network-learning/fives.png)

如果让我们自己来设计一个能将 5 正确分类的程序，就会意外地发现这是一个很难的问题。人可以简单地识别出 5，但却很难明确说出是基于何种规律而识别出了 5。

因此，与其绞尽脑汁，从零开始想出一个可以识别 5 的算法，不如考虑通过有效利用数据来解决这个问题。一种方案是，先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。

![从人工设计规则转变为由机器从数据中学习：没有人为介入的方块用灰色表示](/images/deep-learning/neural-network-learning/human-to-machine.png)

### 训练数据和测试数据

机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验。

首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。

::: info 泛化能力

泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。**获得泛化能力是机器学习的最终目标**。

比如，在识别手写数字的问题中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。此时，手写数字识别就必须具备较高的识别“某个人”写的字的能力。注意这里不是“特定的某个人写的特定的文字”，而是“任意一个人写的任意文字”。如果系统只能正确识别已有的训练数据，那有可能是只学习到了训练数据中的个人的习惯写法。

因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。

:::

## 损失函数

神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。神经网络的学习中所用的指标称为**损失函数**（loss function）。这个损失函数可以使用任意函数，但一般用**均方误差**和**交叉熵误差**等。

### 均方误差

可以用作损失函数的函数有很多，其中最有名的是**均方误差**（mean squared error）。均方误差如下式所示：

$$E = \frac{1}{2} \sum_{k} (y_k - t_k)^2$$

这里，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据，$k$ 表示数据的维数。

均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

::: details 手写数字识别的例子

$y_k$、$t_k$ 是由如下 10 个元素构成的数据：

```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

数组元素的索引从第一个开始依次对应数字 0、1、2 ······ 这里，神经网络的输出 y 是 softmax 函数的输出。由于 softmax 函数的输出可以理解为概率，因此上例表示 0 的概率是 0.1，1 的概率是 0.05，2 的概率是 0.6 等。t 是监督数据，将正确解标签设为 1，其他均设为 0。这里，标签 2 为 1，表示正确解是 2。将正确解标签表示为 1，其他标签表示为 0 的表示方法称为 **one-hot 表示**。

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 设“2”为正确解

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 例1：“2”的概率最高的情况（0.6）
print(mean_squared_error(np.array(y), np.array(t))) # 0.097500000000000031

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 例2：“7”的概率最高的情况（0.6）
print(mean_squared_error(np.array(y), np.array(t))) # 0.59750000000000003
```

这里举了两个例子。第一个例子中，正确解是 2，神经网络的输出的最大值是 2；第二个例子中，正确解是 2，神经网络的输出的最大值是 7。 如实验结果所示，我们发现第一个例子的损失函数的值更小，和监督数据之间的误差较小。也就是说，均方误差显示第一个例子的输出结果与监督数据更加吻合。

:::

### 交叉熵误差

除了均方误差之外，**交叉熵误差**（cross entropy error）也经常被用作损失函数。交叉熵误差如下式所示：

$$E = - \sum_{k} t_k \log y_k$$

这里，log 表示以 e 为底数的自然对数（$log_e$）。 $y_k$ 是神经网络的输出，$t_k$ 是正确解标签。并且，$t_k$ 中只有正确解标签的索引为 1，其他均为 0（one-hot 表 示 ）。

::: tip 提示

该式实际上只计算对应正确解标签的输出的自然对数。

比如，假设正确解标签的索引是 2，与之对应的神经网络的输出是 0.6，则交叉熵误差是 $−log0.6 = 0.51$； 若 2 对应的输出是 0.1，则交叉熵误差为 $−log0.1=2.30$。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。
:::

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

::: details 代码解释

这里，参数 y 和 t 是 NumPy 数组。函数内部在计算 np.log 时，加上了一个微小值 delta。这是因为，当出现 np.log(0) 时，np.log(0) 会变为负无限大的 -inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。

:::

::: details 代码解释

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 设“2”为正确解

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 例1：“2”的概率最高的情况（0.6）
print(cross_entropy_error(np.array(y), np.array(t))) # 0.51082545709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 例2：“7”的概率最高的情况（0.6）
print(cross_entropy_error(np.array(y), np.array(t))) # 2.3025840929945458
```

第一个例子中，正确解标签对应的输出为 0.6，此时的交叉熵误差大约为 0.51。第二个例子中，正确解标签对应的输出为 0.1 的低值，此时的交叉熵误差大约为 2.3。

:::

### 平均损失函数

机器学习使用训练数据进行学习。使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此，计算损失函数时必须将所有的训练数据作为对象。也就是说，如果训练数据有 100 个的话，我们就要把这 100 个损失函数的总和作为学习的指标。

前面介绍的损失函数的例子中考虑的都是针对单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式子：

$$E = -\frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}$$

这里，假设数据有 N 个，$t_{nk}$ 表示第 n 个数据的第 k 个元素的值（$y_{nk}$ 是神经网络的输出，$t_{nk}$ 是监督数据）。

通过除以 N，可以求单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。比如，即便训练数据有 1000 个或 10000 个，也可以求得单个数据的平均损失函数。

### mini-batch 学习

MNIST 数据集的训练数据有 60000 个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。

因此，我们从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为 mini-batch,小批量），然后对每个 mini-batch 进行学习。比如，从 60000 个训练数据中随机选择 100 笔，再用这 100 笔数据进行学习。这种学习方式称为 **mini-batch 学习**。

### mini-batch 版交叉熵误差的实现

::: code-group

```python [同时处理单个数据和批量数据（数据作为batch集中输入）]
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

```python [监督数据是标签形式（非one-hot表示，而是像2、7这样的标签）]
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

:::

实现的要点是，由于 one-hot 表示中 t 为 0 的元素的交叉熵误差也为 0，因此针对这些元素的计算可以忽略。

### 为何要设定损失函数

假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。

此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。

如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。

不过，当导数的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。

::: info 总结

在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。

:::

::: details 为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0 呢？

假设某个神经网络正确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32%。

如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32%，不会出现变化。

也就是说，**仅仅微调参数，是无法改善识别精度的**。即便识别精度有所改善，它的值也不会像 32.0123...%这样连续变化，而是变为 33%、34% 这样的不连续的、离散的值。

而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543...这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像 0.93432...这样发生连续性的变化。

:::

## 数值微分
